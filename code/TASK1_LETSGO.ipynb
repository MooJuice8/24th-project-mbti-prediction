{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 106067 entries, 0 to 106066\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   posts   106067 non-null  object\n",
      " 1   type    106067 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                                                posts  type\n",
       " 0  know intj tool use interaction people excuse a...  INTJ\n",
       " 1  rap music ehh opp yeah know valid well know fa...  INTJ\n",
       " 2  preferably p hd low except wew lad video p min...  INTJ\n",
       " 3  drink like wish could drink red wine give head...  INTJ\n",
       " 4  space program ah bad deal meing freelance max ...  INTJ)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'MBTI_500.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset and its basic information\n",
    "data_info = data.info()\n",
    "data_head = data.head()\n",
    "\n",
    "data_info, data_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenization and train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MoohyeonKim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MoohyeonKim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 1000), (200, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function for preprocessing\n",
    "def preprocess_text(texts):\n",
    "    # Tokenize the text\n",
    "    tokens = [word_tokenize(text.lower()) for text in texts]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_texts = [\" \".join([word for word in token if word not in stop_words]) for token in tokens]\n",
    "    \n",
    "    return filtered_texts\n",
    "\n",
    "# Apply preprocessing on a smaller subset to demonstrate\n",
    "subset_size = 1000  # Using a smaller subset for demonstration\n",
    "data_subset = data.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_texts = preprocess_text(data_subset['posts'])\n",
    "\n",
    "# Vectorize the preprocessed text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for demonstration\n",
    "X = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
    "y = data_subset['type']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection - Comparison of 5 models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MoohyeonKim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision  Recall  F1 Score\n",
      "0     Logistic Regression     0.645   0.605448   0.645  0.591326\n",
      "1  Support Vector Machine     0.695   0.684150   0.695  0.678801\n",
      "2             Naive Bayes     0.435   0.302888   0.435  0.332326\n",
      "3           Random Forest     0.645   0.614778   0.645  0.605259\n",
      "4       Gradient Boosting     0.690   0.718143   0.690  0.694243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Initialize models\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "svm_model = LinearSVC(max_iter=1000)\n",
    "nb_model = MultinomialNB()\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_model,\n",
    "    \"Support Vector Machine\": svm_model,\n",
    "    \"Naive Bayes\": nb_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"Gradient Boosting\": gbm_model\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "    y_pred = model.predict(X_test)  # Predict on test set\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Append results to list\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": fscore\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "results_df = pd.concat([pd.DataFrame([r]) for r in results], ignore_index=True)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 SVM이나 GB를 쓰면 될 듯 함. 또는 이 둘의 앙상블 기법 활용?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:  \n",
    "- Addressing Class Imbalance: If your dataset is imbalanced (some MBTI types have significantly more examples than others), consider techniques like SMOTE for oversampling the minority classes or adjusting class weights in the model.  \n",
    "- Parameter Tuning: Experimenting with different hyperparameters for both the Logistic Regression and SVM models can potentially improve performance further.  \n",
    "- Feature Engineering: Exploring different settings for TF-IDF vectorization (such as adjusting the number of features, using bigrams or trigrams) or even using word embeddings could yield better results.  \n",
    "- Advanced Models: Consider trying more complex models or neural networks, such as those based on the transformer architecture, if you have the computational resources. These models might capture the nuances of language better but require careful tuning and more computational power.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance: Using a language model like GPT-3 or GPT-4 to generate synthetic text data that mimics the writing styles and content preferences of specific MBTI types\n",
    "\n",
    "1. Analyze the Current Dataset Distribution  \n",
    "First, identify which MBTI types are underrepresented in your dataset. This involves analyzing the distribution of MBTI types within your dataset to determine which types need more samples.\n",
    "\n",
    "2. Prepare Seed Prompts for Text Generation  \n",
    "For each underrepresented MBTI type, prepare seed prompts that capture the essence of how individuals with that MBTI type might express themselves. These prompts could be based on characteristics known to be associated with the MBTI type or derived from the existing samples in your dataset.\n",
    "\n",
    "3. Use GPT for Text Generation  \n",
    "With your seed prompts ready, use the OpenAI API to generate additional text samples. You can customize the prompts to encourage the generation of text that reflects specific aspects of the targeted MBTI personality.  \n",
    "For example:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# openai.api_key = 'your-api-key'\n",
    "\n",
    "# response = openai.Completion.create(\n",
    "#   engine=\"text-davinci-003\", # or whichever engine you prefer\n",
    "#   prompt=\"Write a reflective journal entry from the perspective of an INFP talking about their day.\",\n",
    "#   temperature=0.7,\n",
    "#   max_tokens=150,\n",
    "#   top_p=1.0,\n",
    "#   frequency_penalty=0.0,\n",
    "#   presence_penalty=0.0,\n",
    "#   n=5 # Generate multiple samples per prompt\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Post-process and Validate Generated Text  \n",
    "After generating the text, it's important to post-process and validate it to ensure it aligns well with the targeted MBTI type's characteristics. This step might involve manual review or could be assisted by classifiers trained to identify inconsistencies or off-target generations.\n",
    "\n",
    "5. Integrate Synthetic Data  \n",
    "Finally, integrate the synthetic data into your dataset, ensuring a more balanced distribution across MBTI types. Retrain your model on this augmented dataset and evaluate its performance to see if the class imbalance issue has been mitigated effectively.\n",
    "\n",
    "Considerations  \n",
    "Quality Control: Ensure the generated text is of high quality and accurately reflects the nuances of the targeted MBTI type. Poor-quality or irrelevant synthetic data could harm your model's performance.\n",
    "Ethical Use: Be transparent about using synthetic data in your model training process, especially if the model's output will be used in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MBTI Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "INTP    23.533238\n",
       "INTJ    21.144182\n",
       "INFJ    14.107121\n",
       "INFP    11.439939\n",
       "ENTP    11.054334\n",
       "ENFP     5.814249\n",
       "ISTP     3.228148\n",
       "ENTJ     2.785975\n",
       "ESTP     1.872401\n",
       "ENFJ     1.446256\n",
       "ISTJ     1.171901\n",
       "ISFP     0.824950\n",
       "ISFJ     0.612820\n",
       "ESTJ     0.454430\n",
       "ESFP     0.339408\n",
       "ESFJ     0.170647\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the distribution of MBTI types in the dataset\n",
    "mbti_distribution = data['type'].value_counts(normalize=True) * 100\n",
    "\n",
    "mbti_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least represented types: ESFJ, ESFP, ESTJ, and ISFJ (constituting less than 1% of the total entries.)  \n",
    "The most represented types: INTP, INTJ, INFJ, and INFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's generate more samples for mbti types with less than 2%: ESTP, ENFJ, ISTJ, ISFP, ISFJ, ESTJ, ESFP\n",
    "\n",
    "**Example Prompts for MBTI Type Text Generation**  \n",
    "For each MBTI type, you would create a prompt that guides the GPT model to generate text reflecting that type's communication style, interests, and typical expression forms. Here's how you might structure these prompts:  \n",
    "\n",
    "ESTP: \"Write a short story about an ESTP experiencing an exciting adventure in the city, showcasing their spontaneous and action-oriented nature.\"  \n",
    "ENFJ: \"Compose a motivational speech from the perspective of an ENFJ, focusing on inspiring a team to achieve a common goal, demonstrating their empathetic and leadership qualities.\"  \n",
    "ISTJ: \"Draft an email from an ISTJ planning a detailed and structured family reunion, highlighting their organization skills and dedication to tradition.\"  \n",
    "ISFP: \"Describe a day in the life of an ISFP artist, capturing their creative process, love for beauty, and preference for expressing themselves through art.\"  \n",
    "ISFJ: \"Write a diary entry from an ISFJ volunteering at a local community center, reflecting on the day's events and their feelings of satisfaction from helping others.\"  \n",
    "ESTJ: \"Create a detailed plan from an ESTJ organizing a corporate event, emphasizing their leadership, efficiency, and practical problem-solving approach.\"  \n",
    "ESFP: \"Tell a story about an ESFP throwing an impromptu party for their friends, focusing on their spontaneity, love for social gatherings, and ability to live in the moment.\"  \n",
    "ESFJ: \"Write a letter from an ESFJ to a friend going through a tough time, offering support and advice, showcasing their caring, sociable, and supportive nature.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Text with the OpenAI API**  \n",
    "For each prompt, you would use the OpenAI API similar to the following command, adjusting the prompt parameter accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.Completion.create(\n",
    "#   engine=\"text-davinci-003\",\n",
    "#   prompt=\"PROMPT_GOES_HERE\",\n",
    "#   temperature=0.7,\n",
    "#   max_tokens=200,\n",
    "#   top_p=1.0,\n",
    "#   frequency_penalty=0.0,\n",
    "#   presence_penalty=0.0,\n",
    "#   n=5 # Number of samples to generate\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrating Generated Samples**  \n",
    "After generating the samples, carefully review them to ensure they accurately reflect the intended MBTI type's characteristics. You may need to manually verify the quality and relevance of the generated text to ensure it's suitable for training your model.  \n",
    "\n",
    "Once satisfied, append these samples to your dataset, ensuring to label them correctly with their MBTI type. This augmented dataset should then provide a more balanced representation of MBTI types, potentially improving your model's classification performance across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advice on integrating generated samples:  \n",
    "##### Integrating Generated Samples\n",
    "**Review and Clean**: After generating the text samples, it's crucial to review them for relevance and quality. Ensure that the generated text aligns with the characteristics and communication style of the targeted MBTI type. Remove any samples that are off-topic, repetitive, or do not meet the quality standards.  \n",
    "\n",
    "**Label Appropriately**: Assign the correct MBTI type label to each generated sample. This step is crucial for maintaining the integrity of your dataset and ensuring that the model learns the correct associations between text patterns and MBTI types.  \n",
    "\n",
    "**Balance Your Dataset**: Aim for a balanced representation of MBTI types in your dataset. While perfect balance may not be achievable or necessary, reducing the skewness can help improve model performance across all types.  \n",
    "\n",
    "**Split Your Dataset**: After integrating the generated samples, split your dataset into training, validation, and test sets. This split is crucial for evaluating your model's performance on unseen data.  \n",
    "\n",
    "##### Refining Prompts  \n",
    "**Target MBTI Characteristics**: Make sure your prompts are specifically designed to elicit responses that reflect the unique traits of each MBTI type. Researching each type's common behaviors, interests, and communication styles can help craft more effective prompts.  \n",
    "\n",
    "**Vary Prompt Types**: Use a variety of prompt types to generate a diverse set of responses. For instance, besides reflective journal entries or stories, consider prompts that ask for opinions on topics, describe reactions to hypothetical scenarios, or involve planning an event. This variety can help capture a broader spectrum of each type's characteristics.  \n",
    "\n",
    "**Adjust Parameters**: Experiment with different settings for the temperature, max_tokens, and top_p parameters to control the creativity, length, and diversity of the generated responses. A lower temperature (e.g., 0.5-0.7) tends to produce more coherent and predictable text, while a higher temperature (e.g., 0.8-1.0) generates more diverse and creative content.  \n",
    "\n",
    "**Iterative Refinement**: It's a process of trial and error. Generate a small batch of samples, evaluate their quality and relevance, and adjust your prompts based on your findings. This iterative process can help you fine-tune the prompts to produce more accurate and high-quality text samples.  \n",
    "\n",
    "##### Post-Generation Processing  \n",
    "**Augmentation**: Consider using text augmentation techniques on both original and generated samples to further increase the diversity of your dataset. Techniques such as synonym replacement or sentence restructuring can provide additional variance.  \n",
    "\n",
    "**Quality Control Mechanism**: Establish a quality control mechanism, possibly involving manual review or a secondary classifier, to ensure that the generated text meets your criteria for inclusion in the training dataset.  \n",
    "\n",
    "**Ethical Considerations**: When generating and using synthetic text, consider the ethical implications, including transparency about the use of generated text and ensuring that the generated content does not perpetuate biases or stereotypes.  \n",
    "\n",
    "By following these guidelines, you can effectively integrate GPT-generated samples into your dataset to address class imbalance and refine your prompts to ensure the generation of high-quality, relevant text for each MBTI type. This approach can enhance your model's ability to classify MBTI types accurately by providing a richer and more balanced training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Code Example for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Replace \"your_api_key_here\" with your actual OpenAI API key\n",
    "openai.api_key = 'your_api_key_here'\n",
    "\n",
    "def generate_text(prompt, n=5, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"\n",
    "    Generate text using the OpenAI GPT model.\n",
    "\n",
    "    :param prompt: The prompt to generate text for.\n",
    "    :param n: Number of text samples to generate.\n",
    "    :param temperature: Controls the creativity of the output. Higher values mean more creative responses.\n",
    "    :param max_tokens: The maximum number of tokens to generate in the output.\n",
    "    :return: A list of generated text samples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",  # Choose the model version\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            n=n\n",
    "        )\n",
    "        return [completion['text'].strip() for completion in response['choices']]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example prompt for an ESTP type\n",
    "prompt_estp = \"Imagine you're an ESTP talking about your latest adventure with friends. Describe the thrill of the moment and how you navigated the challenges you faced.\"\n",
    "\n",
    "# Generate text for the ESTP prompt\n",
    "generated_texts_estp = generate_text(prompt=prompt_estp, n=5)\n",
    "\n",
    "for i, text in enumerate(generated_texts_estp, start=1):\n",
    "    print(f\"Sample {i}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코드 설명:\n",
    "**Customizing the Function**  \n",
    "prompt: Replace this with any of the MBTI-specific prompts you've prepared.  \n",
    "n: Adjust this to control how many text samples you want to generate per prompt. Generating multiple samples can give you a broader range of responses to evaluate and select from.  \n",
    "temperature: This parameter controls the creativity of the responses. A higher temperature results in more varied and creative outputs, while a lower temperature produces more deterministic and possibly coherent outputs.  \n",
    "max_tokens: This defines the maximum length of the generated text. Adjust this based on how long you want your samples to be.  \n",
    "\n",
    "**Running the Code**  \n",
    "To generate text for different MBTI types, replace the prompt_estp variable with the appropriate prompt for the MBTI type you're focusing on. This code snippet will print out the generated text samples, which you can then review and select for inclusion in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing the performance of machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for Gradient Boosting\n",
    "\n",
    "use GridSearchCV from sklearn.model_selection to systematically work through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid_gbm = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_gbm = GridSearchCV(estimator=gbm, param_grid=param_grid_gbm, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters for Gradient Boosting:\", grid_search_gbm.best_params_)\n",
    "print(\"Best Score for Gradient Boosting:\", grid_search_gbm.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for SVM\n",
    "\n",
    "Similarly, for SVM, you can use GridSearchCV to explore different configurations. Since LinearSVC does not directly expose the kernel parameter, if you're specifically interested in experimenting with kernel SVMs, consider using SVC from sklearn.svm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Type of SVM kernel\n",
    "    'degree': [2, 3, 4],  # Degree of the polynomial kernel function (if 'poly' is chosen)\n",
    "    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "svm = SVC()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"Best Score for SVM:\", grid_search_svm.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips for Hyperparameter Tuning\n",
    "\n",
    "Computational Cost: Hyperparameter tuning, especially grid search with many parameters and large datasets, can be computationally expensive. Consider starting with a smaller subset of your data or a more limited set of parameter values to get a sense of performance trends.  \n",
    "\n",
    "Cross-Validation: Using cross-validation (cv parameter in GridSearchCV) helps ensure that the chosen hyperparameters generalize well to unseen data.  \n",
    "\n",
    "Scoring Metric: Ensure the scoring parameter in GridSearchCV aligns with your project goals (e.g., 'accuracy', 'f1_weighted'). Different metrics will lead to different tuning results.  \n",
    "\n",
    "Parallelization: Setting n_jobs=-1 utilizes all available CPUs to perform the searches in parallel, speeding up the grid search process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Embeddings**: Replace TF-IDF with word embeddings like Word2Vec, GloVe, or fastText. These models can capture semantic meanings of words and are particularly powerful for NLP tasks. You can use pre-trained embeddings or train your own on your dataset.  \n",
    "\n",
    "**n-grams**: Expanding your feature space to include bi-grams or tri-grams (sequences of two or three words) can help capture more context than single words alone, although this will increase the dimensionality of your data.  \n",
    "\n",
    "**Part-of-Speech Tagging**: Adding features based on the part of speech (e.g., noun, verb, adjective) of words in your texts might help the model learn more about the syntactic structure of sentences.  \n",
    "\n",
    "**Sentiment Analysis**: Incorporate features that capture the sentiment of the text. This can be particularly useful if certain MBTI types are more prone to expressing positive or negative sentiments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 사용하기 - 귀찮을거같음 ㅋㅋㅋ  \n",
    "\n",
    "Training Your Own Word2Vec Model  \n",
    "If you prefer to train a Word2Vec model on your own dataset to capture domain-specific semantics, you can do so using Gensim.  \n",
    "\n",
    "Download Pre-trained Word2Vec: Google's pre-trained Word2Vec model is trained on part of the Google News dataset. It contains 300-dimensional vectors for 3 million words and phrases. You can download it from the official Google Code Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare Your Dataset: Tokenize your text data into a list of words for each document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming `data['posts']` is your text column\n",
    "sentences = [word_tokenize(document.lower()) for document in data['posts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train Word2Vec Model: Use Gensim to train a model on your processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create an Embedding Matrix: After training, create an embedding matrix as you would with pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # Match the `vector_size` parameter from the model\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue  # Word not in model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use Embedding Matrix in Neural Network: The embedding matrix can then be used in the embedding layer of a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Flatten())  # Or consider more complex layers like LSTM, GRU, or Conv1D\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=len(set(y_train)), activation='softmax'))  # Assuming y_train contains your labels\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning model -> Advanced Model : BERT 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코랩 T4 GPU 사용한다는 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Steps (Skeleton) generated by GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the Environment\n",
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Preprocess the Data\n",
    "# Load the dataset and preprocess it. Preprocessing includes encoding the MBTI types into numerical labels.\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"/mnt/data/MBTI_500.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Encode MBTI types\n",
    "label_encoder = LabelEncoder()\n",
    "data['type_encoded'] = label_encoder.fit_transform(data['type'])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['posts'], data['type_encoded'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the Text Data\n",
    "# Tokenize the text using BERT's tokenizer.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_data(tokenizer, texts, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "max_length = 128\n",
    "train_inputs, train_masks = encode_data(tokenizer, X_train, max_length)\n",
    "val_inputs, val_masks = encode_data(tokenizer, X_val, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Data Loaders\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "val_labels = torch.tensor(y_val.values)\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load BERT Model for Sequence Classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set Up the Training Loop\n",
    "\n",
    "#Define Optimizer & Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,  # Learning rate\n",
    "                  eps = 1e-8  # Epsilon\n",
    "                 )\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs  # Number of training epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Training & Validation Loop\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Specify the number of epochs\n",
    "epochs = 4\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생각보다 어렵다.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이제 Train 된 모델 (ML이든 BERT든)을 챗봇으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Integrate Model with a Chat Interface**  \n",
    "You'll need a way to interact with users in real-time. This can be done through various platforms, such as a web application, a messaging app (like Telegram, Slack, or Discord), or even a simple command-line interface. Choose the platform that best fits your target audience and the resources you have available.\n",
    "\n",
    "- **Web Application**: Use frameworks like Flask or Django in Python to create a web interface where users can chat with the bot.\n",
    "- **Messaging Apps**: Platforms like Telegram and Slack offer APIs to build bots that can respond to user messages.\n",
    "- **Command-Line Interface (CLI)**: For a simple prototype, a CLI chatbot can be developed where users input their responses directly in the terminal.\n",
    "2. **Preprocess User Input**\n",
    "Just as you preprocessed your training data, user inputs need to be preprocessed before making predictions. This includes tokenization, removing stopwords (if your model requires this), and converting inputs into the format expected by your model (e.g., sequences of tokens for BERT).\n",
    "\n",
    "3. **Make Predictions and Interpret Results**\n",
    "Once the user input is preprocessed and formatted correctly, use your trained model to predict the MBTI type. For BERT and other transformer-based models, this means running the input through the model and interpreting the output logits. For SVM, GB, and other traditional models, the input needs to be vectorized (e.g., with TF-IDF) before making predictions.\n",
    "\n",
    "4. **Respond to the User**\n",
    "Based on the predicted MBTI type, craft responses that the chatbot can return to the user. These responses can include insights about the predicted MBTI type, further questions to refine the prediction, or any other relevant information.\n",
    "\n",
    "5. **Loop and Refine**\n",
    "The chatbot should allow for multiple interactions, refining its predictions based on cumulative responses or simply engaging the user in a conversation about their MBTI type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Workflow for a BERT-based Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the trained model (Assuming it's already trained and saved)\n",
    "model = BertForSequenceClassification.from_pretrained('path_to_saved_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def classify_mbti(text):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Interpret the prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    # Convert prediction to MBTI type (assuming you have a mapping)\n",
    "    mbti_type = label_encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    return mbti_type\n",
    "\n",
    "# Example chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    mbti_type = classify_mbti(user_input)\n",
    "    print(f\"Chatbot: Based on your response, your MBTI type might be {mbti_type}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, for SVM or GB models, you'll need to vectorize the user input using the same transformation applied to the training data (e.g., TF-IDF) before making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Touches**  \n",
    "\n",
    "**User Experience**: Consider the user experience. Ensure responses are friendly, informative, and engaging.  \n",
    "**Testing and Iteration**: Test your chatbot extensively to refine its accuracy and user interaction. Gather feedback to make improvements.  \n",
    "**Deployment**: Choose a deployment platform. Cloud services like AWS, GCP, and Azure offer scalable options for deploying models and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
